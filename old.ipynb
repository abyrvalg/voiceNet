{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VNet():\n",
    "    def __init__(self, layers, unit_type=\"bin\", noise_std = 0.001, model_path='./models/dbm.ckpt'):\n",
    "        tf.reset_default_graph()\n",
    "        self.unit_type = unit_type\n",
    "        self.layers_nums = layers\n",
    "        self.layers = []\n",
    "        self.noise_std = noise_std\n",
    "        self.input = tf.placeholder(\"bool\" if self.unit_type == \"bin\" else 'float', [None, None], name=\"input\")\n",
    "        self.model_path = model_path\n",
    "        self.loaded_model = False\n",
    "        self.vae_initialized = False\n",
    "        \n",
    "        i = 0\n",
    "        for unit_count in layers:\n",
    "            self.layers.append({\n",
    "                \"state\" : None,\n",
    "                \"b\" : tf.Variable(tf.zeros([unit_count]), name=\"b_\"+str(i))\n",
    "            })\n",
    "            i+=1\n",
    "        self.W = []\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.W.append(tf.Variable(tf.random_normal((layers[i], layers[i+1]), mean=0.0, stddev=0.01), name=\"W_\"+str(i)))\n",
    "        \n",
    "        self.tf_saver = tf.train.Saver()\n",
    "        \n",
    "    def __gaussian_noise_layer(self, x):\n",
    "        noise = tf.random_normal(shape=tf.shape(x), mean=0.0, stddev=self.noise_std, dtype=tf.float32) \n",
    "        return x + noise\n",
    "    \n",
    "    def sample_forward(self, start_from=0, steps_num=None):\n",
    "        if(steps_num == None or steps_num > len(self.layers)):\n",
    "            steps_num = len(self.layers)-1\n",
    "        for i in range(start_from, start_from + steps_num):\n",
    "            mul = tf.matmul(self.layers[i][\"state\"], self.W[i]) + self.layers[i+1][\"b\"]\n",
    "            \n",
    "            if(self.unit_type == \"bin\") :\n",
    "                self.layers[i+1][\"state\"] = tf.nn.relu(tf.sign(tf.sigmoid(mul) \n",
    "                                                               - np.random.rand(tf.shape(mul)[0], tf.shape(mul)[1])))\n",
    "            else:\n",
    "                 self.layers[i+1][\"state\"] = tf.nn.relu(self.__gaussian_noise_layer(mul))\n",
    "                    \n",
    "        return self.layers[i+1][\"state\"]\n",
    "    \n",
    "    def sample_back(self, start_from=None, steps_count=None):\n",
    "        if(steps_count == None):\n",
    "            steps_count = len(self.layers)-1\n",
    "            start_from = len(self.layers)-1\n",
    "        i = start_from\n",
    "        while(i > start_from - steps_count):\n",
    "            mul = tf.matmul(self.layers[i][\"state\"], tf.transpose(self.W[i-1])) + self.layers[i-1][\"b\"]\n",
    "            if(self.unit_type == \"bin\"):\n",
    "                self.layers[i-1][\"state\"] = tf.nn.relu(tf.sign(tf.sigmoid(mul) \n",
    "                                                               - np.random.rand(tf.shape(mul)[0], tf.shape(mul)[1])))\n",
    "            else:\n",
    "                 self.layers[i-1][\"state\"] = self.__gaussian_noise_layer(mul)\n",
    "            i -= 1\n",
    "        return self.layers[i][\"state\"]\n",
    "    \n",
    "    def __getNextBatch(self, batch_size):\n",
    "        if(self.train_set):\n",
    "            if(callable(self.train_set)):\n",
    "                gen = self.train_set(batch_size)\n",
    "                for result in gen:\n",
    "                    yield result\n",
    "                    \n",
    "    def prepare_train_set(self, batch_size, epochs_count):\n",
    "        if(callable(self.train_set)):\n",
    "            return self.train_set(self.files, int(self.layers_nums[0]), batch_size, epochs_count, True)\n",
    "    \n",
    "    def train_dbm(self, train_set, batch_size, learning_rate, epochs_count, decrease_noise = 0, pcd = 1, depth=None):\n",
    "        if(depth == None or depth > len(self.layers)-1):\n",
    "            depth = len(self.layers)\n",
    "        self.train_set = train_set\n",
    "        if(self.loaded_model == False):\n",
    "            self.tf_sess = tf.Session()\n",
    "            self.tf_sess.run(tf.global_variables_initializer())\n",
    "        self.train_set = self.prepare_train_set(batch_size, epochs_count*(depth-1))\n",
    "        print(\"start training \"+str(learning_rate))\n",
    "        for i in range(depth-1):\n",
    "            self.noise_std = 0.001\n",
    "            inp = self.train_set.__next__()\n",
    "            for j in  range(epochs_count):\n",
    "                self.noise_std = self.noise_std - decrease_noise*self.noise_std\n",
    "                self.layers[0][\"state\"] = self.input\n",
    "                h0_state = self.sample_forward(0, i+1)\n",
    "                v0_state = self.layers[i][\"state\"]\n",
    "                positive = tf.matmul(tf.transpose(v0_state), h0_state)\n",
    "                for k in range(pcd):                    \n",
    "                    v1_state = self.sample_back(i+1, 1)\n",
    "                    h1_state = self.sample_forward(i, 1)\n",
    "                    negative = tf.matmul(tf.transpose(v1_state), h1_state)\n",
    "                    w_update = self.W[i].assign_add(learning_rate*(positive - negative))\n",
    "                    v_loss = tf.reduce_mean(v0_state - v1_state, 0)\n",
    "                    vb_update = self.layers[i][\"b\"].assign_add(learning_rate*(v_loss))\n",
    "                    hb_update = self.layers[i+1][\"b\"].assign_add(learning_rate*(tf.reduce_mean(h0_state - h1_state, 0)))\n",
    "                    w_upd, bv_upd, bh_upd, loss = self.tf_sess.run([w_update, vb_update, hb_update, v_loss], feed_dict={\n",
    "                        self.input : norm(inp)\n",
    "                    })\n",
    "                if(j % 100 == 0):\n",
    "                    print(\"epoch: \"+str(j)+\" layer:\"+str(i)+\" loss:\")\n",
    "                    print(np.mean(np.power(loss, 2)))\n",
    "        self.train_set.__next__()\n",
    "        self.tf_saver.save(self.tf_sess, self.model_path)\n",
    "        \n",
    "    def load_model(self, path=None):\n",
    "        self.tf_sess = tf.Session()\n",
    "        self.tf_sess.run(tf.global_variables_initializer())\n",
    "        self.tf_saver.restore(self.tf_sess, path if path != None else self.model_path)\n",
    "        self.loaded_model = True\n",
    "        \n",
    "    def encode(self, data):\n",
    "        self.layers[0][\"state\"] = self.input\n",
    "        res = self.vae_recognize() if self.vae_initialized == True else self.sample_forward()\n",
    "        with tf.Session() as self.tf_sess:\n",
    "            self.tf_sess.run(tf.global_variables_initializer())\n",
    "            self.tf_saver.restore(self.tf_sess, self.model_path)\n",
    "            return self.tf_sess.run(res, feed_dict={\n",
    "                self.input : data\n",
    "            })\n",
    "    \n",
    "    def decode(self, data):        \n",
    "        if(self.vae_initialized == True):\n",
    "            self.z = self.input\n",
    "            res = self.vae_generate()\n",
    "        else:\n",
    "            self.layers[-1][\"state\"] = self.input\n",
    "            res = self.sample_back()\n",
    "        with tf.Session() as self.tf_sess:\n",
    "            self.tf_sess.run(tf.global_variables_initializer())\n",
    "            self.tf_saver.restore(self.tf_sess, self.model_path)\n",
    "            return self.tf_sess.run(res, feed_dict={\n",
    "                self.input : data\n",
    "            })\n",
    "        \n",
    "    def xavier_init(self, fan_in, fan_out, constant=1): \n",
    "        low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "        high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "        return tf.random_uniform((fan_in, fan_out), \n",
    "                                 minval=low, maxval=high, \n",
    "                                 dtype=tf.float32)\n",
    "\n",
    "    def init_vae(self, latent_size):\n",
    "        print(self.vae_initialized)\n",
    "        if(self.vae_initialized != True):\n",
    "            print(\"init vae\")\n",
    "            self.latent_size = latent_size\n",
    "            self.decode_layers = []\n",
    "            for i in range(len(self.layers)-1):\n",
    "                self.decode_layers.append({\n",
    "                    \"state\" : None,\n",
    "                    \"b\" : self.layers[i][\"b\"]\n",
    "                })\n",
    "                \n",
    "            self.decode_W = []\n",
    "            for i in range(len(self.layers)-1):\n",
    "                self.decode_W.append(tf.transpose(self.W[i]))\n",
    "            self.z_in_mean_W = self.xavier_init(self.layers_nums[-1], latent_size);\n",
    "            self.z_in_mean_b = tf.Variable(tf.zeros([latent_size], dtype=tf.float32));\n",
    "            self.z_out_mean_W = self.xavier_init(latent_size, self.layers_nums[-1]);\n",
    "            self.z_out_mean_b = tf.Variable(tf.zeros([self.layers_nums[-1]], dtype=tf.float32));\n",
    "            self.z_signa_squared_W = self.xavier_init(self.layers_nums[-1], latent_size);\n",
    "            self.z_signa_squared_b = tf.Variable(tf.zeros([latent_size], dtype=tf.float32))\n",
    "            self.class_w = self.xavier_init(latent_size, len(self.files));\n",
    "            self.class_b = tf.Variable(tf.zeros([len(self.files)], dtype=tf.float32))\n",
    "            self.vae_initialized = True\n",
    "    \n",
    "    def vae_recognize(self):\n",
    "        encoded = self.sample_forward()\n",
    "        self.z_mean = tf.matmul(encoded, self.z_in_mean_W) + self.z_in_mean_b\n",
    "        self.z_signa_squared = tf.matmul(encoded, self.z_signa_squared_W) + self.z_signa_squared_b\n",
    "        \n",
    "        eps = tf.random_normal((self.batch_size, self.latent_size), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        # z = mu + sigma*epsilon\n",
    "        return tf.add(self.z_mean, \n",
    "                        tf.multiply(tf.sqrt(tf.exp(self.z_signa_squared )), eps))\n",
    "        \n",
    "    def vae_generate(self):\n",
    "        self.layers[-1][\"state\"] = tf.matmul(self.z, self.z_out_mean_W)+self.z_out_mean_b\n",
    "        return self.sample_back()\n",
    "        \n",
    "    def train_vae(self, train_set, batch_size, learning_rate, epochs_count):\n",
    "        self.train_set = train_set\n",
    "        self.layers[0][\"state\"] = self.input\n",
    "        self.batch_size = batch_size\n",
    "        self.z = self.vae_recognize()\n",
    "        reconstruction = self.vae_generate()\n",
    "        reconstr_loss = tf.reduce_mean(tf.square(reconstruction-self.input))\n",
    "        \n",
    "        latent_loss = -0.0001 * tf.reduce_sum(1 + self.z_signa_squared \n",
    "                                           - tf.square(self.z_mean) \n",
    "                                           - tf.exp(self.z_signa_squared), 1)\n",
    "        self.cost = tf.reduce_mean(reconstr_loss + latent_loss)   # average over batch\n",
    "        mimmaze = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
    "        self.train_set = self.prepare_train_set(batch_size, epochs_count)\n",
    "        print(\"start vae training\");\n",
    "        with tf.Session() as self.tf_sess:\n",
    "            self.tf_sess.run(tf.global_variables_initializer())\n",
    "            self.tf_saver.restore(self.tf_sess, self.model_path)\n",
    "            for i in range(epochs_count):                \n",
    "                if(i % 100 == 0):\n",
    "                    cost, minimize = self.tf_sess.run([self.cost, mimmaze], feed_dict={\n",
    "                        self.input : norm(self.train_set.__next__())\n",
    "                    })\n",
    "                    print(\"epoch: \"+str(i)+\" loss:\"+str(cost))\n",
    "                else:\n",
    "                    self.tf_sess.run(mimmaze, feed_dict={\n",
    "                        self.input : norm(self.train_set.__next__())\n",
    "                    })\n",
    "            self.train_set.__next__()\n",
    "            self.tf_saver.save(self.tf_sess, self.model_path)\n",
    "    \n",
    "    def regression(self, train_set, batch_size, epoch_count):\n",
    "        for i in range(len(self.layers_nums)):\n",
    "            tf.stop_gradient(self.layers[i][\"b\"])\n",
    "            if(i < len(self.layers_nums)-1):\n",
    "                tf.stop_gradient(self.W[i])\n",
    "        file_labels = tf.placeholder(tf.int64, [None], name=\"Labels\")\n",
    "        self.train_set = train_set\n",
    "        if(self.loaded_model == False):\n",
    "            self.load_model()\n",
    "        self.train_set = self.prepare_train_set(batch_size, epoch_count*(len(self.layers)), True)\n",
    "        for j in range(epoch_count):\n",
    "            self.layers[0][\"state\"] = self.input\n",
    "            res = self.sample_forward()\n",
    "            mul = tf.matmul(res, self.rW) + self.rb\n",
    "            cross_entropy = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=mul, labels=file_labels))\n",
    "            train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)\n",
    "            batch = self.train_set.__next__()\n",
    "            err, step = self.tf_sess.run([cross_entropy, train_step], feed_dict = {\n",
    "                self.input : norm(batch[0]),\n",
    "                file_labels : batch[1]\n",
    "            })\n",
    "            if(j % 100 == 0):\n",
    "                print(\"epoch: \"+str(j)+\" loss:\")\n",
    "                print(err)\n",
    "        self.train_set.__next__()\n",
    "        self.tf_saver.save(self.tf_sess, self.model_path)\n",
    "\n",
    "    def transform(self, inp, target, k):\n",
    "        self.layers[0][\"state\"] = self.input\n",
    "        encoded = self.sample_forward()\n",
    "        self.layers[-1][\"state\"] = encoded + k*tf.transpose(self.rW)[target]\n",
    "        res = self.sample_back()\n",
    "        if(self.loaded_model == False):\n",
    "            self.load_model()\n",
    "        enc, res = self.tf_sess.run([encoded, res], feed_dict={\n",
    "            self.input : inp\n",
    "        })\n",
    "        print(self.tf_sess.run(tf.transpose(self.rW)[target]))\n",
    "        return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
