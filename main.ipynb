{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, architecture, files, model_path='./models/dbm.ckpt'):\n",
    "        self.model_path = model_path\n",
    "        self.architecture = architecture\n",
    "        self.transfer_fct = tf.nn.relu\n",
    "        self.files = files\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, [None, architecture[\"input\"]])\n",
    "        \n",
    "        self._int_params(architecture)\n",
    "        self._create_loss_optimizer()\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        self.tf_saver = tf.train.Saver()\n",
    "        \n",
    "    def load_model(self, path=None):\n",
    "        self.tf_sess = tf.Session()\n",
    "        self.tf_sess.run(tf.global_variables_initializer())\n",
    "        self.tf_saver.restore(self.tf_sess, path if path != None else self.model_path)\n",
    "        self.loaded_model = True\n",
    "        \n",
    "    def normalize(x, get_mu_std=False):\n",
    "        mean, var = tf.nn.moments(x, axes=[0])\n",
    "        if(get_mu_std):\n",
    "            return (x - mu)/var, mu, var\n",
    "        else:\n",
    "            return (x - mu)/var\n",
    "\n",
    "    def restore(x, mu, var):\n",
    "        return (x*var + mu) - tf.reduce_min(x)\n",
    "        \n",
    "    def get_data_generator(file_path, frame_with, frame_count, batch_count, rand=False, get_meta=False, get_label=False):\n",
    "        if(type(file_path ) == str):\n",
    "            wave_read = [wave.open(file_path, \"rb\")]\n",
    "        else:\n",
    "            wave_read = []\n",
    "            for fp in file_path:\n",
    "                wave_read.append (wave.open(fp, \"rb\"))\n",
    "        if(get_meta):\n",
    "            meta = {\n",
    "                \"nframes\" : wave_read[0].getnframes(),\n",
    "                \"nchannels\" : wave_read[0].getnchannels(),\n",
    "                \"sampwidth\" : wave_read[0].getsampwidth(),\n",
    "                \"framerate\" : wave_read[0].getframerate()\n",
    "            }\n",
    "        for k in range(batch_count):\n",
    "            out = []\n",
    "            labels = []\n",
    "            for i in range(frame_count):\n",
    "                label = np.random.randint(len(wave_read)) - 1\n",
    "                chosen_file = wave_read[label]\n",
    "                if rand:\n",
    "                    chosen_file.setpos(np.random.randint(chosen_file.getnframes() - frame_with))           \n",
    "                if(get_label == True):\n",
    "                    labels.append(label)\n",
    "                out.append(np.fromstring(chosen_file.readframes(frame_with), np.uint16))\n",
    "            yield [out, labels] if(get_label == True) else out\n",
    "        for fr in wave_read:\n",
    "            fr.close()\n",
    "        yield meta if get_meta else None\n",
    "        \n",
    "    def train(self, epoch_count, batch_size, learning_rate):\n",
    "        data_generator = self.get_data_generator(self.files, self.architecture['input'], batch_size, epoch_count, True, False, True)\n",
    "        \n",
    "        normal_x = self.normalize(self.x)\n",
    "        z, z_mean, z_log_sigma_sq = self.encode(normal_x)\n",
    "        reconstruct = self.decode(z)\n",
    "        reconstr_loss = tf.reduce_mean(tf.pow(normal_x - reconstruct, 2))\n",
    "        latent_loss = -0.001 * tf.reduce_sum(1 + z_log_sigma_sq \n",
    "                                           - tf.square(z_mean) \n",
    "                                           - tf.exp(z_log_sigma_sq), 1)\n",
    "        \n",
    "        class_y =  tf.placeholder(tf.float32, [None, len(self.files)])\n",
    "        class_loss = 0.1 * tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=tf.matmul(z, self.params['class']['W']) \n",
    "                                                               + self.params['class']['b'], labels=class_y))\n",
    "        \n",
    "        self.cost = tf.reduce_mean(reconstr_loss + latent_loss + class_loss)\n",
    "\n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(learning_rate= learning_rate).minimize(self.cost)\n",
    "        self.tf_sess = tf.Session()  \n",
    "        for i in range(epoch_count):\n",
    "            optimizer = self.tf_sess.run(self.optimizer, feed_dict={self.x: data_generator.__next__(), y: train_targets})\n",
    "        \n",
    "        data_generator.__next__()   \n",
    "        self.tf_saver.save(self.tf_sess, self.model_path)\n",
    "        \n",
    "    def encode(self, data):\n",
    "        z_mean, z_log_sigma_sq = self._forward_pass(self.params['rec'], data)\n",
    "        eps = tf.random_normal((self.batch_size, self.architecture['z']), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        return tf.add(z_mean, tf.multiply(tf.sqrt(tf.exp(z_log_sigma_sq)), eps)), z_mean, z_log_sigma_sq\n",
    "    \n",
    "    def decode(self, data):\n",
    "        return self._forward_pass(self.params['gen'], data)\n",
    "            \n",
    "    def _int_params(self, architecture):\n",
    "        params = {\n",
    "            'rec' : {\n",
    "                'W' : {\n",
    "                    'layers' : [],\n",
    "                    'mean' : tf.Variable(xavier_init(architecture['rec'][-1], architecture['z'])),\n",
    "                    'log_sigma' : tf.Variable(xavier_init(architecture['rec'][-1], architecture['z']))\n",
    "                },\n",
    "                'b' : {\n",
    "                    'layers' : [],\n",
    "                    'mean' : tf.Variable(tf.zeros([architecture['z']], dtype=tf.float32)),\n",
    "                    'log_sigma' : tf.Variable(tf.zeros([architecture['z']], dtype=tf.float32))\n",
    "                }\n",
    "            },\n",
    "            'gen' : {\n",
    "                'W' : {\n",
    "                    'layers' : [],\n",
    "                    'mean' : tf.Variable(xavier_init(architecture['gen'][-1], architecture['z'])),\n",
    "                    'log_sigma' : tf.Variable(xavier_init(architecture['gen'][-1], architecture['z']))\n",
    "                },\n",
    "                'b' : {\n",
    "                    'layers' : [],\n",
    "                    'mean' : tf.Variable(tf.zeros([architecture['input']], dtype=tf.float32)),\n",
    "                    'log_sigma' : tf.Variable(tf.zeros([architecture['input']], dtype=tf.float32))\n",
    "                }\n",
    "            },\n",
    "            'class' : {\n",
    "                'W' : tf.Variable(xavier_init(architecture['z'], len(self.files))),\n",
    "                'b' : tf.Variable(tf.zeros([architecture['input']], dtype=tf.float32))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for i in range(len(architecture['rec'])):\n",
    "            if(isinstance(architecture[\"rec\"][i], list)):\n",
    "                print('capsule to implement')   \n",
    "            else:\n",
    "                params['rec']['W']['layers'].append(tf.Variable(xavier_init(\\\n",
    "                    architecture[\"input\"] if i==0 else architecture['rec'][i-1], \\\n",
    "                    architecture[\"rec\"][i])))            \n",
    "                params['rec']['b']['layers'].append(tf.Variable(tf.zeros([architecture[\"rec\"][i]], dtype=tf.float32)))\n",
    "             \n",
    "        params['rec']['W']['mean'] = tf.Variable(xavier_init(architecture['rec'][i], architecture['z']))\n",
    "        params['rec']['W']['log_sigma'] = tf.Variable(xavier_init(architecture['rec'][i], architecture['z']))\n",
    "        \n",
    "        for i in range(len(architecture['gen'])):\n",
    "            if(isinstance(architecture[\"gen\"][i], list)):\n",
    "                print('capsule to implement')   \n",
    "            else:\n",
    "                params['gen']['W']['layers'].append(tf.Variable(xavier_init(\\\n",
    "                    architecture[\"z\"] if i==0 else architecture['gen'][i-1], \\\n",
    "                    architecture[\"gen\"][i])))            \n",
    "                params['rec']['b']['layers'].append(tf.Variable(tf.zeros([architecture[\"gen\"][i]], dtype=tf.float32)))\n",
    "             \n",
    "        params['rec']['W']['mean'] = tf.Variable(xavier_init(architecture['gen'][i], architecture['input']))\n",
    "        params['rec']['W']['log_sigma'] = tf.Variable(xavier_init(architecture['gen'][i], architecture['input']))\n",
    "        \n",
    "        return params\n",
    "            \n",
    "    def _forward_pass(self, params, data):\n",
    "        current_val = data\n",
    "        for i in range(len(params['W']['layers'])):\n",
    "            current_val = self.transfer_fct(tf.add(tf.matmul(current_val, params['W'][i]), \n",
    "                                           params['b']['layers'])) \n",
    "\n",
    "        z_mean = tf.add(tf.matmul(current_val, params['W']['mean']), params['b']['mean'])\n",
    "        \n",
    "        if(params['W']['log_sigma']):\n",
    "            return (z_mean, tf.add(tf.matmul(layer_2, params['W']['log_sigma']), params['b']['out_log_sigma']))\n",
    "        else:\n",
    "            return z_mean\n",
    "            \n",
    "    def _create_loss_optimizer(self):\n",
    "        # The loss is composed of two terms:\n",
    "        # 1.) The reconstruction loss (the negative log probability\n",
    "        #     of the input under the reconstructed Bernoulli distribution \n",
    "        #     induced by the decoder in the data space).\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for reconstructing the input when the activation in latent\n",
    "        #     is given.\n",
    "        # Adding 1e-10 to avoid evaluation of log(0.0)\n",
    "        reconstr_loss = tf.reduce_mean(tf.pow(self.x - self.x_reconstr_mean, 2))\n",
    "        # 2.) The latent loss, which is defined as the Kullback Leibler divergence \n",
    "        ##    between the distribution in latent space induced by the encoder on \n",
    "        #     the data and some prior. This acts as a kind of regularizer.\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for transmitting the the latent space distribution given\n",
    "        #     the prior.\n",
    "        latent_loss = -0.001 * tf.reduce_sum(1 + self.z_log_sigma_sq \n",
    "                                           - tf.square(self.z_mean) \n",
    "                                           - tf.exp(self.z_log_sigma_sq), 1)\n",
    "        self.cost = tf.reduce_mean(reconstr_loss + latent_loss)   # average over batch\n",
    "\n",
    "        # Use ADAM optimizer\n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "        \n",
    "    def partial_fit(self, X):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "        \n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "        opt, cost = self.sess.run((self.optimizer, self.cost), \n",
    "                                  feed_dict={self.x: X})\n",
    "        return cost\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data by mapping it into the latent space.\"\"\"\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self.sess.run(self.z_mean, feed_dict={self.x: X})\n",
    "    \n",
    "    def generate(self, z_mu=None):\n",
    "        \"\"\" Generate data by sampling from latent space.\n",
    "        \n",
    "        If z_mu is not None, data for this point in latent space is\n",
    "        generated. Otherwise, z_mu is drawn from prior in latent \n",
    "        space.        \n",
    "        \"\"\"\n",
    "        if z_mu is None:\n",
    "            z_mu = np.random.normal(size=self.network_architecture[\"z\"])\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.z: z_mu})\n",
    "    \n",
    "    def reconstruct(self, X):\n",
    "        \"\"\" Use VAE to reconstruct given data. \"\"\"\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.x: X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def norm(x, get_mu_std=False):\n",
    "    ar = np.array(x)\n",
    "    mu = np.mean(ar)\n",
    "    std = np.std(ar)\n",
    "    if(get_mu_std):\n",
    "        return (ar - mu)/std, mu, std\n",
    "    else:\n",
    "        return (ar - mu)/std\n",
    "\n",
    "def restore(x, mu, std):\n",
    "    return np.array(x)*std + mu\n",
    "\n",
    "def get_file_frame(file_path, frame_with, frame_count, batch_count, rand=False, get_meta=False, get_label=False):\n",
    "    if(type(file_path ) == str):\n",
    "        wave_read = [wave.open(file_path, \"rb\")]\n",
    "    else:\n",
    "        wave_read = []\n",
    "        for fp in file_path:\n",
    "            wave_read.append (wave.open(fp, \"rb\"))\n",
    "    if(get_meta):\n",
    "        meta = {\n",
    "            \"nframes\" : wave_read[0].getnframes(),\n",
    "            \"nchannels\" : wave_read[0].getnchannels(),\n",
    "            \"sampwidth\" : wave_read[0].getsampwidth(),\n",
    "            \"framerate\" : wave_read[0].getframerate()\n",
    "        }\n",
    "    for k in range(batch_count):\n",
    "        out = []\n",
    "        labels = []\n",
    "        for i in range(frame_count):\n",
    "            label = np.random.randint(len(wave_read)) - 1\n",
    "            chosen_file = wave_read[label]\n",
    "            if rand:\n",
    "                chosen_file.setpos(np.random.randint(chosen_file.getnframes() - frame_with))           \n",
    "            if(get_label == True):\n",
    "                labels.append(label)\n",
    "            out.append(np.fromstring(chosen_file.readframes(frame_with), np.uint16))\n",
    "        yield [out, labels] if(get_label == True) else out\n",
    "    for fr in wave_read:\n",
    "        fr.close()\n",
    "    yield meta if get_meta else None\n",
    "    \n",
    "def write_wave(array, path, meta, mu=None, std=None):\n",
    "    unrolled = array.ravel()\n",
    "    writer = wave.open(path, \"wb\")\n",
    "    writer.setnframes(meta[\"nframes\"])\n",
    "    if(mu != None and std!=None):\n",
    "        unrolled = restore(unrolled, mu, std)\n",
    "    bytes_arr = np.rint(unrolled).astype(np.uint16).tobytes()\n",
    "    writer.setnchannels(meta[\"nchannels\"])\n",
    "    writer.setsampwidth(meta[\"sampwidth\"])\n",
    "    writer.setframerate(meta[\"framerate\"])\n",
    "    writer.writeframes(bytes_arr)\n",
    "    writer.close()\n",
    "\n",
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "init vae\n"
     ]
    }
   ],
   "source": [
    "vnet = VNet([500, 300, 200, 100], \"gauss\")\n",
    "vnet.files= [\"01.wav\", \"03.wav\", \"04.wav\", \"05.wav\", \"06.wav\", \"07.wav\", \"11.wav\" ,\"13.wav\", \"14.wav\"]\n",
    "vnet.load_model()\n",
    "vnet.noise_std = 0\n",
    "vnet.init_vae(70)\n",
    "#rbm.train(get_file_frame, 500, 0.0000015, 700, 0.002, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vnet.train_vae(get_file_frame, 500, 0.00001, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 50)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vnet = VNet([500, 300, 200, 100], \"gauss\")\n",
    "#vnet.load_model()\n",
    "gen = get_file_frame(\"01.wav\", 500, 500, 1, False, True)\n",
    "foo = gen.__next__()  \n",
    "foo, mu, std = norm(foo, True)\n",
    "res = vnet.encode(foo) \n",
    "meta = gen.__next__()\n",
    "np.shape(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "decoded = vnet.decode(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "write_wave(decoded, \"ress.wav\", meta, mu, std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "network_architecture = {\n",
    "    \"input\":1000,\n",
    "    \"rec\" : [700, 500, 300, 150],\n",
    "    \"gen\" : [100, 200, 500, 700],\n",
    "    \"z\" : 100\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'x_reconstr_mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-526-e559eea2e9c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork_architecture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"01.wav\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"03.wav\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"04.wav\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"05.wav\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"06.wav\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"07.wav\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"11.wav\"\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m\"13.wav\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"14.wav\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-525-30a97f9738e9>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, architecture, files, model_path)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_int_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marchitecture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_loss_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-525-30a97f9738e9>\u001b[0m in \u001b[0;36m_create_loss_optimizer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;31m#     is given.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;31m# Adding 1e-10 to avoid evaluation of log(0.0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[0mreconstr_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_reconstr_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m         \u001b[1;31m# 2.) The latent loss, which is defined as the Kullback Leibler divergence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;31m##    between the distribution in latent space induced by the encoder on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Model' object has no attribute 'x_reconstr_mean'"
     ]
    }
   ],
   "source": [
    "model = Model(network_architecture, [\"01.wav\", \"03.wav\", \"04.wav\", \"05.wav\", \"06.wav\", \"07.wav\", \"11.wav\" ,\"13.wav\", \"14.wav\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen = get_file_frame( [\"01.wav\", \"03.wav\", \"04.wav\", \"05.wav\", \"06.wav\", \"07.wav\", \"11.wav\" ,\"13.wav\", \"14.wav\"], 500, 500, 10000, rand=True, get_meta=False, get_label=False);\n",
    "def train(network_architecture, learning_rate=0.0001,\n",
    "          batch_size=500, training_epochs=10000, display_step=100):\n",
    "    vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 batch_size=batch_size)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(n_samples / batch_size)\n",
    "        # Loop over all batches\n",
    "        \n",
    "        batch_xs= norm(gen.__next__());\n",
    "        # Fit training using batch data\n",
    "        cost = vae.partial_fit(batch_xs)\n",
    "        # Compute average loss\n",
    "        avg_cost += cost / batch_size\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \n",
    "                  \"cost=\", \"{:.9f}\".format(cost))\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.163512707\n",
      "Epoch: 0101 cost= 1.020631671\n",
      "Epoch: 0201 cost= 1.008952618\n",
      "Epoch: 0301 cost= 1.000304461\n",
      "Epoch: 0401 cost= 0.976029575\n",
      "Epoch: 0501 cost= 0.940716863\n",
      "Epoch: 0601 cost= 0.899858057\n",
      "Epoch: 0701 cost= 0.852550328\n",
      "Epoch: 0801 cost= 0.826211929\n",
      "Epoch: 0901 cost= 0.799576521\n",
      "Epoch: 1001 cost= 0.753931284\n",
      "Epoch: 1101 cost= 0.752783835\n",
      "Epoch: 1201 cost= 0.738080144\n",
      "Epoch: 1301 cost= 0.716445208\n",
      "Epoch: 1401 cost= 0.717823088\n",
      "Epoch: 1501 cost= 0.681948721\n",
      "Epoch: 1601 cost= 0.673137486\n",
      "Epoch: 1701 cost= 0.649670243\n",
      "Epoch: 1801 cost= 0.663625360\n",
      "Epoch: 1901 cost= 0.661385357\n",
      "Epoch: 2001 cost= 0.639850795\n",
      "Epoch: 2101 cost= 0.648124099\n",
      "Epoch: 2201 cost= 0.619280875\n",
      "Epoch: 2301 cost= 0.621437252\n",
      "Epoch: 2401 cost= 0.600855112\n",
      "Epoch: 2501 cost= 0.610756814\n",
      "Epoch: 2601 cost= 0.624804914\n",
      "Epoch: 2701 cost= 0.598593235\n",
      "Epoch: 2801 cost= 0.597748160\n",
      "Epoch: 2901 cost= 0.590362430\n",
      "Epoch: 3001 cost= 0.583675206\n",
      "Epoch: 3101 cost= 0.615513563\n",
      "Epoch: 3201 cost= 0.593134105\n",
      "Epoch: 3301 cost= 0.576188862\n",
      "Epoch: 3401 cost= 0.592859983\n",
      "Epoch: 3501 cost= 0.593038082\n",
      "Epoch: 3601 cost= 0.573537409\n",
      "Epoch: 3701 cost= 0.582889020\n",
      "Epoch: 3801 cost= 0.564054966\n",
      "Epoch: 3901 cost= 0.564874768\n",
      "Epoch: 4001 cost= 0.558335066\n",
      "Epoch: 4101 cost= 0.562172353\n",
      "Epoch: 4201 cost= 0.547923207\n",
      "Epoch: 4301 cost= 0.573930681\n",
      "Epoch: 4401 cost= 0.557348430\n",
      "Epoch: 4501 cost= 0.547414541\n",
      "Epoch: 4601 cost= 0.545754313\n",
      "Epoch: 4701 cost= 0.567891359\n",
      "Epoch: 4801 cost= 0.558131933\n",
      "Epoch: 4901 cost= 0.533610225\n",
      "Epoch: 5001 cost= 0.573738754\n",
      "Epoch: 5101 cost= 0.544797838\n",
      "Epoch: 5201 cost= 0.548920095\n",
      "Epoch: 5301 cost= 0.546693504\n",
      "Epoch: 5401 cost= 0.545474350\n",
      "Epoch: 5501 cost= 0.527048469\n",
      "Epoch: 5601 cost= 0.546561539\n",
      "Epoch: 5701 cost= 0.549166918\n",
      "Epoch: 5801 cost= 0.556470037\n",
      "Epoch: 5901 cost= 0.549681723\n",
      "Epoch: 6001 cost= 0.564663351\n",
      "Epoch: 6101 cost= 0.538860142\n",
      "Epoch: 6201 cost= 0.550013781\n",
      "Epoch: 6301 cost= 0.549548745\n",
      "Epoch: 6401 cost= 0.550111473\n",
      "Epoch: 6501 cost= 0.539553404\n",
      "Epoch: 6601 cost= 0.549347281\n",
      "Epoch: 6701 cost= 0.523448408\n",
      "Epoch: 6801 cost= 0.555354834\n",
      "Epoch: 6901 cost= 0.539948106\n",
      "Epoch: 7001 cost= 0.544115186\n",
      "Epoch: 7101 cost= 0.552981317\n",
      "Epoch: 7201 cost= 0.532313168\n",
      "Epoch: 7301 cost= 0.531057358\n",
      "Epoch: 7401 cost= 0.540373027\n",
      "Epoch: 7501 cost= 0.530603647\n",
      "Epoch: 7601 cost= 0.518394172\n",
      "Epoch: 7701 cost= 0.550463378\n",
      "Epoch: 7801 cost= 0.536683857\n",
      "Epoch: 7901 cost= 0.528322875\n",
      "Epoch: 8001 cost= 0.535355270\n",
      "Epoch: 8101 cost= 0.543573737\n",
      "Epoch: 8201 cost= 0.539738536\n",
      "Epoch: 8301 cost= 0.548035145\n",
      "Epoch: 8401 cost= 0.517731488\n",
      "Epoch: 8501 cost= 0.532650590\n",
      "Epoch: 8601 cost= 0.547729671\n",
      "Epoch: 8701 cost= 0.525521219\n",
      "Epoch: 8801 cost= 0.512188673\n",
      "Epoch: 8901 cost= 0.524463534\n",
      "Epoch: 9001 cost= 0.529945314\n",
      "Epoch: 9101 cost= 0.542333603\n",
      "Epoch: 9201 cost= 0.539014637\n",
      "Epoch: 9301 cost= 0.525558233\n",
      "Epoch: 9401 cost= 0.538084507\n",
      "Epoch: 9501 cost= 0.532470644\n",
      "Epoch: 9601 cost= 0.521578014\n",
      "Epoch: 9701 cost= 0.534813583\n",
      "Epoch: 9801 cost= 0.521984518\n",
      "Epoch: 9901 cost= 0.538616598\n"
     ]
    }
   ],
   "source": [
    "vae = train(network_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen = get_file_frame(\"01.wav\", 500, 500, 1, False, True)\n",
    "foo = gen.__next__()  \n",
    "foo, mu, std = norm(foo, True)\n",
    "res = vae.reconstruct(foo)\n",
    "meta = gen.__next__()\n",
    "write_wave(res, \"ress.wav\", meta, mu, std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen = get_file_frame(\"01.wav\", 500, 500, 1, False, True)\n",
    "foo1 = gen.__next__()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta = gen.__next__()\n",
    "foo = np.add(foo1, 10000)\n",
    "write_wave(foo, \"ress.wav\", meta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[64535, -1000, -1000, ..., -1000, -1000, -1000],\n",
       "       [-1000, -1000, -1000, ..., -1000, -1000, -1000],\n",
       "       [-1000, 64535, 64535, ..., 64534, 64531, 64528],\n",
       "       ..., \n",
       "       [64510, 64513, 64517, ..., 64502, 64502, 64500],\n",
       "       [64496, 64494, 64491, ..., 64533, 64533, 64529],\n",
       "       [64529, 64526, 64523, ..., 64497, 64503, 64505]], dtype=int32)"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
