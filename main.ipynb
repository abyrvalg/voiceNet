{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DBM():\n",
    "    def __init__(self, layers, unit_type=\"bin\", noise_std = 0.001, model_path='./models/dbm.ckpt'):\n",
    "        tf.reset_default_graph()\n",
    "        self.unit_type = unit_type\n",
    "        self.layers_nums = layers\n",
    "        self.layers = []\n",
    "        self.noise_std = noise_std\n",
    "        self.input = tf.placeholder(\"bool\" if self.unit_type == \"bin\" else 'float', [None, None], name=\"input\")\n",
    "        self.model_path = model_path\n",
    "        self.loaded_model = False\n",
    "        \n",
    "        i = 0\n",
    "        for unit_count in layers:\n",
    "            self.layers.append({\n",
    "                \"state\" : None,\n",
    "                \"b\" : tf.Variable(tf.zeros([unit_count]), name=\"b_\"+str(i))\n",
    "            })\n",
    "            i+=1\n",
    "        self.W = []\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.W.append(tf.Variable(tf.random_normal((layers[i], layers[i+1]), mean=0.0, stddev=0.01), name=\"W_\"+str(i)))\n",
    "        \n",
    "        self.tf_saver = tf.train.Saver()\n",
    "        \n",
    "    def __gaussian_noise_layer(self, x):\n",
    "        noise = tf.random_normal(shape=tf.shape(x), mean=0.0, stddev=self.noise_std, dtype=tf.float32) \n",
    "        return x + noise\n",
    "    \n",
    "    def sample_forward(self, start_from=0, steps_num=None):\n",
    "        if(steps_num == None or steps_num > len(self.layers)):\n",
    "            steps_num = len(self.layers)-1\n",
    "        for i in range(start_from, start_from + steps_num):\n",
    "            mul = tf.matmul(self.layers[i][\"state\"], self.W[i]) + self.layers[i+1][\"b\"]\n",
    "            \n",
    "            if(self.unit_type == \"bin\") :\n",
    "                self.layers[i+1][\"state\"] = tf.nn.relu(tf.sign(tf.sigmoid(mul) \n",
    "                                                               - np.random.rand(tf.shape(mul)[0], tf.shape(mul)[1])))\n",
    "            else:\n",
    "                 self.layers[i+1][\"state\"] = tf.nn.relu(self.__gaussian_noise_layer(mul))\n",
    "                    \n",
    "        return self.layers[i+1][\"state\"]\n",
    "    \n",
    "    def sample_back(self, start_from=None, steps_count=None):\n",
    "        if(steps_count == None):\n",
    "            steps_count = len(self.layers)-1\n",
    "            start_from = len(self.layers)-1\n",
    "        i = start_from\n",
    "        while(i > start_from - steps_count):\n",
    "            mul = tf.matmul(self.layers[i][\"state\"], tf.transpose(self.W[i-1])) + self.layers[i-1][\"b\"]\n",
    "            if(self.unit_type == \"bin\"):\n",
    "                self.layers[i-1][\"state\"] = tf.nn.relu(tf.sign(tf.sigmoid(mul) \n",
    "                                                               - np.random.rand(tf.shape(mul)[0], tf.shape(mul)[1])))\n",
    "            else:\n",
    "                 self.layers[i-1][\"state\"] = self.__gaussian_noise_layer(mul)\n",
    "            i -= 1\n",
    "        return self.layers[i][\"state\"]\n",
    "    \n",
    "    def __getNextBatch(self, batch_size):\n",
    "        if(self.train_set):\n",
    "            if(callable(self.train_set)):\n",
    "                gen = self.train_set(batch_size)\n",
    "                for result in gen:\n",
    "                    yield result\n",
    "                    \n",
    "    def prepare_train_set(self, batch_size, epochs_count):\n",
    "        if(callable(self.train_set)):\n",
    "            return self.train_set(self.files, int(self.layers_nums[0]), batch_size, epochs_count, True)\n",
    "    \n",
    "    def train(self, train_set, batch_size, learning_rate, epochs_count, decrease_noise = 0, pcd = 1, depth=None):\n",
    "        if(depth == None or depth > len(self.layers)-1):\n",
    "            depth = len(self.layers)\n",
    "        self.train_set = train_set\n",
    "        if(self.loaded_model == False):\n",
    "            self.tf_sess = tf.Session()\n",
    "            self.tf_sess.run(tf.global_variables_initializer())\n",
    "        self.train_set = self.prepare_train_set(batch_size, epochs_count*(depth-1))\n",
    "        print(\"start training \"+str(learning_rate))\n",
    "        for i in range(depth-1):\n",
    "            self.noise_std = 0.001\n",
    "            inp = self.train_set.__next__()\n",
    "            for j in  range(epochs_count):\n",
    "                self.noise_std = self.noise_std - decrease_noise*self.noise_std\n",
    "                self.layers[0][\"state\"] = self.input\n",
    "                h0_state = self.sample_forward(0, i+1)\n",
    "                v0_state = self.layers[i][\"state\"]\n",
    "                positive = tf.matmul(tf.transpose(v0_state), h0_state)\n",
    "                for k in range(pcd):                    \n",
    "                    v1_state = self.sample_back(i+1, 1)\n",
    "                    h1_state = self.sample_forward(i, 1)\n",
    "                    negative = tf.matmul(tf.transpose(v1_state), h1_state)\n",
    "                    w_update = self.W[i].assign_add(learning_rate*(positive - negative))\n",
    "                    v_loss = tf.reduce_mean(v0_state - v1_state, 0)\n",
    "                    vb_update = self.layers[i][\"b\"].assign_add(learning_rate*(v_loss))\n",
    "                    hb_update = self.layers[i+1][\"b\"].assign_add(learning_rate*(tf.reduce_mean(h0_state - h1_state, 0)))\n",
    "                    w_upd, bv_upd, bh_upd, loss = self.tf_sess.run([w_update, vb_update, hb_update, v_loss], feed_dict={\n",
    "                        self.input : norm(inp)\n",
    "                    })\n",
    "                if(j % 100 == 0):\n",
    "                    print(\"epoch: \"+str(j)+\" layer:\"+str(i)+\" loss:\")\n",
    "                    print(np.mean(np.power(loss, 2)))\n",
    "        self.train_set.__next__()\n",
    "        self.tf_saver.save(self.tf_sess, self.model_path)\n",
    "        \n",
    "    def load_model(self, path=None):\n",
    "        self.tf_sess = tf.Session()\n",
    "        self.tf_sess.run(tf.global_variables_initializer())\n",
    "        self.tf_saver.restore(self.tf_sess, path if path != None else self.model_path)\n",
    "        self.loaded_model = True\n",
    "        \n",
    "    def encode(self, data):\n",
    "        self.layers[0][\"state\"] = self.input\n",
    "        res = self.sample_forward()\n",
    "        with tf.Session() as self.tf_sess:\n",
    "           # print(\"WAT\")\n",
    "            self.tf_sess.run(tf.global_variables_initializer())\n",
    "            self.tf_saver.restore(self.tf_sess, self.model_path)\n",
    "            return self.tf_sess.run(res, feed_dict={\n",
    "                self.input : data\n",
    "            })\n",
    "    \n",
    "    def decode(self, data):\n",
    "        self.layers[-1][\"state\"] = self.input\n",
    "        res = self.sample_back()\n",
    "        with tf.Session() as self.tf_sess:\n",
    "            self.tf_sess.run(tf.global_variables_initializer())\n",
    "            self.tf_saver.restore(self.tf_sess, self.model_path)\n",
    "            return self.tf_sess.run(res, feed_dict={\n",
    "                self.input : data\n",
    "            })\n",
    "        \n",
    "    def loss(self):\n",
    "        self.layers[0][\"state\"] = self.input\n",
    "        return np.mean(np.pow(self.sample_back(self.sample_forvard(data) - self.input)))\n",
    "    \n",
    "    def finetune(self, train_set, batch_size, learning_rate, epochs_count):\n",
    "        self.train_set = train_set\n",
    "        if(self.decode_layers !=None):\n",
    "            self.decode_layers = []\n",
    "            for i in range(len(layers)-1):\n",
    "                self.decode_layers.append({\n",
    "                    \"state\" : None,\n",
    "                    \"b\" : self.layers[i][\"b\"]\n",
    "                })\n",
    "                i+=1\n",
    "\n",
    "            decode_W.W = []\n",
    "            for i in range(len(layers)-1):\n",
    "                self.decode_W.append(tf.transpose(self.W[i]))\n",
    "                \n",
    "        mimmaze = tf.train.AdamOptimizer(learning_rate).minimize(self.loss())\n",
    "        self.train_set = self.prepare_train_set(batch_size, len(self.layers))\n",
    "        \n",
    "        with tf.Session() as self.tf_sess:\n",
    "            self.tf_sess.run(tf.global_variables_initializer())\n",
    "            self.tf_saver.restore(self.tf_sess, self.model_path)\n",
    "            for i in range(epochs_count):\n",
    "                 self.tf_sess.run(mimmaze, feed_dict={\n",
    "                    self.input : norm(self.train_set.__next__())\n",
    "                })\n",
    "                \n",
    "    def compute_loss(self, data):\n",
    "        return np.mean(np.pow(self.decode(self.encode(data) - data)))\n",
    "    \n",
    "    def regression(self, train_set, batch_size, epoch_count):\n",
    "        for i in range(len(self.layers_nums)):\n",
    "            tf.stop_gradient(self.layers[i][\"b\"])\n",
    "            if(i < len(self.layers_nums)-1):\n",
    "                tf.stop_gradient(self.W[i])\n",
    "        file_labels = tf.placeholder(tf.int64, [None], name=\"Labels\")\n",
    "        self.train_set = train_set\n",
    "        if(self.loaded_model == False):\n",
    "            self.load_model()\n",
    "        self.train_set = self.prepare_train_set(batch_size, epoch_count*(len(self.layers)), True)\n",
    "        for j in range(epoch_count):\n",
    "            self.layers[0][\"state\"] = self.input\n",
    "            res = self.sample_forward()\n",
    "            mul = tf.matmul(res, self.rW) + self.rb\n",
    "            cross_entropy = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=mul, labels=file_labels))\n",
    "            train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)\n",
    "            batch = self.train_set.__next__()\n",
    "            err, step = self.tf_sess.run([cross_entropy, train_step], feed_dict = {\n",
    "                self.input : norm(batch[0]),\n",
    "                file_labels : batch[1]\n",
    "            })\n",
    "            if(j % 100 == 0):\n",
    "                print(\"epoch: \"+str(j)+\" loss:\")\n",
    "                print(err)\n",
    "        self.train_set.__next__()\n",
    "        self.tf_saver.save(self.tf_sess, self.model_path)\n",
    "\n",
    "    def transform(self, inp, target, k):\n",
    "        self.layers[0][\"state\"] = self.input\n",
    "        encoded = self.sample_forward()\n",
    "        self.layers[-1][\"state\"] = encoded + k*tf.transpose(self.rW)[target]\n",
    "        res = self.sample_back()\n",
    "        if(self.loaded_model == False):\n",
    "            self.load_model()\n",
    "        enc, res = self.tf_sess.run([encoded, res], feed_dict={\n",
    "            self.input : inp\n",
    "        })\n",
    "        print(self.tf_sess.run(tf.transpose(self.rW)[target]))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def norm(x, get_mu_std=False):\n",
    "    ar = np.array(x)\n",
    "    mu = np.mean(ar)\n",
    "    std = np.std(ar)\n",
    "    if(get_mu_std):\n",
    "        return (ar - mu)/std, mu, std\n",
    "    else:\n",
    "        return (ar - mu)/std\n",
    "\n",
    "def restore(x, mu, std):\n",
    "    return np.array(x)*std + mu\n",
    "\n",
    "def get_file_frame(file_path, frame_with, frame_count, batch_count, rand=False, get_meta=False, get_label=False):\n",
    "    if(type(file_path ) == str):\n",
    "        wave_read = [wave.open(file_path, \"rb\")]\n",
    "    else:\n",
    "        wave_read = []\n",
    "        for fp in file_path:\n",
    "            wave_read.append (wave.open(fp, \"rb\"))\n",
    "    if(get_meta):\n",
    "        meta = {\n",
    "            \"nframes\" : wave_read[0].getnframes(),\n",
    "            \"nchannels\" : wave_read[0].getnchannels(),\n",
    "            \"sampwidth\" : wave_read[0].getsampwidth(),\n",
    "            \"framerate\" : wave_read[0].getframerate()\n",
    "        }\n",
    "    for k in range(batch_count):\n",
    "        out = []\n",
    "        labels = []\n",
    "        for i in range(frame_count):\n",
    "            label = np.random.randint(len(wave_read)) - 1\n",
    "            chosen_file = wave_read[label]\n",
    "            if rand:\n",
    "                chosen_file.setpos(np.random.randint(chosen_file.getnframes() - frame_with))           \n",
    "            if(get_label == True):\n",
    "                labels.append(label)\n",
    "            out.append(np.fromstring(chosen_file.readframes(frame_with), np.uint16))\n",
    "        yield [out, labels] if(get_label == True) else out\n",
    "    for fr in wave_read:\n",
    "        fr.close()\n",
    "    yield meta if get_meta else None\n",
    "    \n",
    "def write_wave(array, path, meta, mu=None, std=None):\n",
    "    unrolled = array.ravel()\n",
    "    writer = wave.open(path, \"wb\")\n",
    "    writer.setnframes(meta[\"nframes\"])\n",
    "    unrolled = restore(unrolled, mu, std)\n",
    "    bytes_arr = np.rint(unrolled).astype(np.uint16).tobytes()\n",
    "    writer.setnchannels(meta[\"nchannels\"])\n",
    "    writer.setsampwidth(meta[\"sampwidth\"])\n",
    "    writer.setframerate(meta[\"framerate\"])\n",
    "    writer.writeframes(bytes_arr)\n",
    "    writer.close()\n",
    "\n",
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training 1.5e-06\n",
      "epoch: 0 layer:0 loss:\n",
      "0.00274478\n",
      "epoch: 100 layer:0 loss:\n",
      "0.000312765\n",
      "epoch: 200 layer:0 loss:\n",
      "9.32763e-05\n",
      "epoch: 300 layer:0 loss:\n",
      "6.26744e-05\n",
      "epoch: 400 layer:0 loss:\n",
      "4.18432e-05\n",
      "epoch: 500 layer:0 loss:\n",
      "2.7766e-05\n",
      "epoch: 600 layer:0 loss:\n",
      "1.87814e-05\n",
      "epoch: 0 layer:1 loss:\n",
      "0.297422\n",
      "epoch: 100 layer:1 loss:\n",
      "8.00387e-05\n",
      "epoch: 200 layer:1 loss:\n",
      "5.87564e-05\n",
      "epoch: 300 layer:1 loss:\n",
      "4.25602e-05\n",
      "epoch: 400 layer:1 loss:\n",
      "3.51944e-05\n",
      "epoch: 500 layer:1 loss:\n",
      "2.75689e-05\n",
      "epoch: 600 layer:1 loss:\n",
      "2.22493e-05\n",
      "epoch: 0 layer:2 loss:\n",
      "0.522653\n",
      "epoch: 100 layer:2 loss:\n",
      "6.86326e-05\n",
      "epoch: 200 layer:2 loss:\n",
      "4.10274e-05\n",
      "epoch: 300 layer:2 loss:\n",
      "3.07687e-05\n",
      "epoch: 400 layer:2 loss:\n",
      "2.77465e-05\n",
      "epoch: 500 layer:2 loss:\n",
      "2.55883e-05\n",
      "epoch: 600 layer:2 loss:\n",
      "2.33229e-05\n"
     ]
    }
   ],
   "source": [
    "rbm = DBM([500, 300, 200, 100], \"gauss\")\n",
    "rbm.files= [\"01.wav\", \"03.wav\", \"04.wav\", \"05.wav\", \"06.wav\", \"07.wav\", \"11.wav\" ,\"13.wav\", \"14.wav\"]\n",
    "rbm.train(get_file_frame, 500, 0.0000015, 700, 0.002, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rbm = DBM([500, 300, 200, 100], \"gauss\")\n",
    "rbm.load_model()\n",
    "gen = get_file_frame(\"01.wav\", 500, 500, 1, False, True)\n",
    "foo = gen.__next__()  \n",
    "foo, mu, std = norm(foo, True)\n",
    "res = rbm.encode(foo) \n",
    "meta = gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "decoded = rbm.decode(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "write_wave(decoded, \"ress.wav\", meta, mu, std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(object):\n",
    "    \"\"\" Variation Autoencoder (VAE) with an sklearn-like interface implemented using TensorFlow.\n",
    "    \n",
    "    This implementation uses probabilistic encoders and decoders using Gaussian \n",
    "    distributions and  realized by multi-layer perceptrons. The VAE can be learned\n",
    "    end-to-end.\n",
    "    \n",
    "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, network_architecture, transfer_fct= tf.nn.relu, \n",
    "                 learning_rate=0.001, batch_size=100):\n",
    "        self.network_architecture = network_architecture\n",
    "        self.transfer_fct = transfer_fct\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # tf Graph input\n",
    "        self.x = tf.placeholder(tf.float32, [None, network_architecture[\"n_input\"]])\n",
    "        \n",
    "        # Create autoencoder network\n",
    "        self._create_network()\n",
    "        # Define loss function based variational upper-bound and \n",
    "        # corresponding optimizer\n",
    "        self._create_loss_optimizer()\n",
    "        \n",
    "        # Initializing the tensor flow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Launch the session\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def _create_network(self):\n",
    "        # Initialize autoencode network weights and biases\n",
    "        network_weights = self._initialize_weights(**self.network_architecture)\n",
    "\n",
    "        # Use recognition network to determine mean and \n",
    "        # (log) variance of Gaussian distribution in latent\n",
    "        # space\n",
    "        self.z_mean, self.z_log_sigma_sq = \\\n",
    "            self._recognition_network(network_weights[\"weights_recog\"], \n",
    "                                      network_weights[\"biases_recog\"])\n",
    "\n",
    "        # Draw one sample z from Gaussian distribution\n",
    "        n_z = self.network_architecture[\"n_z\"]\n",
    "        eps = tf.random_normal((self.batch_size, n_z), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        # z = mu + sigma*epsilon\n",
    "        self.z = tf.add(self.z_mean, \n",
    "                        tf.multiply(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n",
    "\n",
    "        # Use generator to determine mean of\n",
    "        # Bernoulli distribution of reconstructed input\n",
    "        self.x_reconstr_mean = \\\n",
    "            self._generator_network(network_weights[\"weights_gener\"],\n",
    "                                    network_weights[\"biases_gener\"])\n",
    "            \n",
    "    def _initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2, \n",
    "                            n_hidden_gener_1,  n_hidden_gener_2, \n",
    "                            n_input, n_z):\n",
    "        all_weights = dict()\n",
    "        all_weights['weights_recog'] = {\n",
    "            'h1': tf.Variable(xavier_init(n_input, n_hidden_recog_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_recog_1, n_hidden_recog_2)),\n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_recog_2, n_z)),\n",
    "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_recog_2, n_z))}\n",
    "        all_weights['biases_recog'] = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\n",
    "        all_weights['weights_gener'] = {\n",
    "            'h1': tf.Variable(xavier_init(n_z, n_hidden_gener_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_gener_1, n_hidden_gener_2)),\n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_gener_2, n_input)),\n",
    "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_gener_2, n_input))}\n",
    "        all_weights['biases_gener'] = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_gener_2], dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_input], dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_input], dtype=tf.float32))}\n",
    "        return all_weights\n",
    "            \n",
    "    def _recognition_network(self, weights, biases):\n",
    "        # Generate probabilistic encoder (recognition network), which\n",
    "        # maps inputs onto a normal distribution in latent space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        z_mean = tf.add(tf.matmul(layer_2, weights['out_mean']),\n",
    "                        biases['out_mean'])\n",
    "        z_log_sigma_sq = \\\n",
    "            tf.add(tf.matmul(layer_2, weights['out_log_sigma']), \n",
    "                   biases['out_log_sigma'])\n",
    "        return (z_mean, z_log_sigma_sq)\n",
    "\n",
    "    def _generator_network(self, weights, biases):\n",
    "        # Generate probabilistic decoder (decoder network), which\n",
    "        # maps points in latent space onto a Bernoulli distribution in data space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.z, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        x_reconstr_mean = (tf.add(tf.matmul(layer_2, weights['out_mean']), \n",
    "                                 biases['out_mean']))\n",
    "        return x_reconstr_mean\n",
    "            \n",
    "    def _create_loss_optimizer(self):\n",
    "        # The loss is composed of two terms:\n",
    "        # 1.) The reconstruction loss (the negative log probability\n",
    "        #     of the input under the reconstructed Bernoulli distribution \n",
    "        #     induced by the decoder in the data space).\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for reconstructing the input when the activation in latent\n",
    "        #     is given.\n",
    "        # Adding 1e-10 to avoid evaluation of log(0.0)\n",
    "        reconstr_loss = tf.reduce_mean(tf.pow(self.x - self.x_reconstr_mean, 2))\n",
    "        # 2.) The latent loss, which is defined as the Kullback Leibler divergence \n",
    "        ##    between the distribution in latent space induced by the encoder on \n",
    "        #     the data and some prior. This acts as a kind of regularizer.\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for transmitting the the latent space distribution given\n",
    "        #     the prior.\n",
    "        latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \n",
    "                                           - tf.square(self.z_mean) \n",
    "                                           - tf.exp(self.z_log_sigma_sq), 1)\n",
    "        self.cost = tf.reduce_mean(reconstr_loss + latent_loss)   # average over batch\n",
    "\n",
    "        # Use ADAM optimizer\n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "        \n",
    "    def partial_fit(self, X):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "        \n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "        opt, cost = self.sess.run((self.optimizer, self.cost), \n",
    "                                  feed_dict={self.x: X})\n",
    "        return cost\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data by mapping it into the latent space.\"\"\"\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self.sess.run(self.z_mean, feed_dict={self.x: X})\n",
    "    \n",
    "    def generate(self, z_mu=None):\n",
    "        \"\"\" Generate data by sampling from latent space.\n",
    "        \n",
    "        If z_mu is not None, data for this point in latent space is\n",
    "        generated. Otherwise, z_mu is drawn from prior in latent \n",
    "        space.        \n",
    "        \"\"\"\n",
    "        if z_mu is None:\n",
    "            z_mu = np.random.normal(size=self.network_architecture[\"n_z\"])\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.z: z_mu})\n",
    "    \n",
    "    def reconstruct(self, X):\n",
    "        \"\"\" Use VAE to reconstruct given data. \"\"\"\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.x: X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True], dtype=bool)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([2,0,3]) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "network_architecture = \\\n",
    "    dict(n_hidden_recog_1=500, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=300, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=300, # 1st layer decoder neurons\n",
    "         n_hidden_gener_2=500, # 2nd layer decoder neurons\n",
    "         n_input=500, # MNIST data input (img shape: 28*28)\n",
    "         n_z=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen = get_file_frame([\"01.wav\"], 500, 500, 10000, rand=True, get_meta=False, get_label=False);\n",
    "def train(network_architecture, learning_rate=0.0005,\n",
    "          batch_size=500, training_epochs=10000, display_step=100):\n",
    "    vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 batch_size=batch_size)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(n_samples / batch_size)\n",
    "        # Loop over all batches\n",
    "        \n",
    "        batch_xs= norm(gen.__next__());\n",
    "        # Fit training using batch data\n",
    "        cost = vae.partial_fit(batch_xs)\n",
    "        # Compute average loss\n",
    "        avg_cost += cost / batch_size\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \n",
    "                  \"cost=\", \"{:.9f}\".format(cost))\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 32.197929382\n",
      "Epoch: 0101 cost= 1.019004464\n",
      "Epoch: 0201 cost= 1.008387685\n",
      "Epoch: 0301 cost= 1.005912304\n",
      "Epoch: 0401 cost= 1.002282619\n",
      "Epoch: 0501 cost= 1.001438498\n",
      "Epoch: 0601 cost= 1.001357436\n",
      "Epoch: 0701 cost= 1.001116037\n",
      "Epoch: 0801 cost= 1.000467181\n",
      "Epoch: 0901 cost= 1.000377774\n",
      "Epoch: 1001 cost= 1.001035333\n",
      "Epoch: 1101 cost= 1.000309229\n",
      "Epoch: 1201 cost= 1.000334382\n",
      "Epoch: 1301 cost= 1.000373721\n",
      "Epoch: 1401 cost= 1.000386000\n",
      "Epoch: 1501 cost= 1.000252485\n",
      "Epoch: 1601 cost= 1.000172615\n",
      "Epoch: 1701 cost= 1.000199556\n",
      "Epoch: 1801 cost= 1.000069857\n",
      "Epoch: 1901 cost= 1.000148892\n",
      "Epoch: 2001 cost= 1.000207424\n",
      "Epoch: 2101 cost= 1.000118613\n",
      "Epoch: 2201 cost= 1.000210524\n",
      "Epoch: 2301 cost= 1.000092030\n",
      "Epoch: 2401 cost= 1.000321150\n",
      "Epoch: 2501 cost= 1.000034928\n",
      "Epoch: 2601 cost= 1.000141144\n",
      "Epoch: 2701 cost= 0.999970675\n",
      "Epoch: 2801 cost= 0.999992907\n",
      "Epoch: 2901 cost= 1.000103831\n",
      "Epoch: 3001 cost= 1.000164866\n",
      "Epoch: 3101 cost= 1.000364065\n",
      "Epoch: 3201 cost= 1.000015855\n",
      "Epoch: 3301 cost= 1.000053287\n",
      "Epoch: 3401 cost= 1.000311971\n",
      "Epoch: 3501 cost= 1.000063062\n",
      "Epoch: 3601 cost= 1.000018239\n",
      "Epoch: 3701 cost= 1.000058055\n",
      "Epoch: 3801 cost= 1.000011563\n",
      "Epoch: 3901 cost= 1.000126123\n",
      "Epoch: 4001 cost= 1.000084519\n",
      "Epoch: 4101 cost= 1.000015497\n",
      "Epoch: 4201 cost= 1.000009656\n",
      "Epoch: 4301 cost= 1.000075459\n",
      "Epoch: 4401 cost= 1.000108600\n",
      "Epoch: 4501 cost= 1.000039220\n",
      "Epoch: 4601 cost= 1.000135064\n",
      "Epoch: 4701 cost= 1.000015736\n",
      "Epoch: 4801 cost= 1.000031114\n",
      "Epoch: 4901 cost= 1.000049114\n",
      "Epoch: 5001 cost= 1.000040054\n",
      "Epoch: 5101 cost= 1.000005841\n",
      "Epoch: 5201 cost= 0.999989152\n",
      "Epoch: 5301 cost= 1.000039816\n",
      "Epoch: 5401 cost= 1.000051498\n",
      "Epoch: 5501 cost= 1.000042558\n",
      "Epoch: 5601 cost= 1.000005722\n",
      "Epoch: 5701 cost= 1.000022531\n",
      "Epoch: 5801 cost= 1.000059485\n",
      "Epoch: 5901 cost= 0.999988377\n",
      "Epoch: 6001 cost= 1.000043869\n",
      "Epoch: 6101 cost= 0.999998510\n",
      "Epoch: 6201 cost= 1.000053525\n",
      "Epoch: 6301 cost= 0.999994338\n",
      "Epoch: 6401 cost= 0.999946237\n",
      "Epoch: 6501 cost= 1.000067115\n",
      "Epoch: 6601 cost= 1.000087619\n",
      "Epoch: 6701 cost= 0.999999523\n",
      "Epoch: 6801 cost= 0.999971271\n",
      "Epoch: 6901 cost= 1.000032425\n",
      "Epoch: 7001 cost= 0.999994159\n",
      "Epoch: 7101 cost= 1.000045657\n",
      "Epoch: 7201 cost= 1.000030994\n",
      "Epoch: 7301 cost= 1.000007153\n",
      "Epoch: 7401 cost= 0.999986708\n",
      "Epoch: 7501 cost= 1.000037670\n",
      "Epoch: 7601 cost= 0.999993920\n",
      "Epoch: 7701 cost= 1.000014782\n",
      "Epoch: 7801 cost= 0.999962330\n",
      "Epoch: 7901 cost= 1.000038505\n",
      "Epoch: 8001 cost= 0.999986589\n",
      "Epoch: 8101 cost= 0.999992788\n",
      "Epoch: 8201 cost= 0.999978602\n",
      "Epoch: 8301 cost= 0.999992251\n",
      "Epoch: 8401 cost= 1.000078082\n",
      "Epoch: 8501 cost= 0.999994338\n",
      "Epoch: 8601 cost= 1.000005841\n",
      "Epoch: 8701 cost= 1.000051975\n",
      "Epoch: 8801 cost= 1.000005722\n",
      "Epoch: 8901 cost= 1.000059485\n",
      "Epoch: 9001 cost= 1.000008106\n",
      "Epoch: 9101 cost= 1.000111341\n",
      "Epoch: 9201 cost= 1.000017405\n",
      "Epoch: 9301 cost= 0.999977171\n",
      "Epoch: 9401 cost= 1.000070095\n",
      "Epoch: 9501 cost= 1.000049829\n",
      "Epoch: 9601 cost= 1.000030041\n",
      "Epoch: 9701 cost= 1.000017166\n",
      "Epoch: 9801 cost= 1.000021458\n",
      "Epoch: 9901 cost= 1.000043154\n"
     ]
    }
   ],
   "source": [
    "vae = train(network_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen = get_file_frame(\"01.wav\", 500, 500, 1, False, True)\n",
    "foo = gen.__next__()  \n",
    "foo, mu, std = norm(foo, True)\n",
    "res = vae.reconstruct(foo)\n",
    "meta = gen.__next__()\n",
    "write_wave(res, \"ress.wav\", meta, mu, std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00384825,  0.00057701,  0.00421493, ...,  0.00760673,\n",
       "         0.00468805,  0.00405899],\n",
       "       [ 0.00384825,  0.00057701,  0.00421493, ...,  0.00760673,\n",
       "         0.00468805,  0.00405899],\n",
       "       [ 0.00384825,  0.00057701,  0.00421493, ...,  0.00760673,\n",
       "         0.00468805,  0.00405899],\n",
       "       ..., \n",
       "       [ 0.00384825,  0.00057701,  0.00421493, ...,  0.00760673,\n",
       "         0.00468805,  0.00405899],\n",
       "       [ 0.00384825,  0.00057701,  0.00421493, ...,  0.00760673,\n",
       "         0.00468805,  0.00405899],\n",
       "       [ 0.00384825,  0.00057701,  0.00421493, ...,  0.00760673,\n",
       "         0.00468805,  0.00405899]], dtype=float32)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
