{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DBM():\n",
    "    def __init__(self, layers, unit_type=\"bin\", noise_std = 0.001, model_path='./models/dbm.ckpt'):\n",
    "        self.unit_type = unit_type\n",
    "        self.layers_nums = layers\n",
    "        self.layers = []\n",
    "        self.noise_std = noise_std\n",
    "        self.input = tf.placeholder(\"bool\" if self.unit_type == \"bin\" else 'float', [None, None], name=\"input\")\n",
    "        self.model_path = model_path\n",
    "        self.loaded_model = False\n",
    "        \n",
    "        i = 0\n",
    "        for unit_count in layers:\n",
    "            self.layers.append({\n",
    "                \"state\" : None,\n",
    "                \"b\" : tf.Variable(tf.zeros([unit_count]), name=\"b_\"+str(i))\n",
    "            })\n",
    "            i+=1\n",
    "        self.W = []\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.W.append(tf.Variable(tf.random_normal((layers[i], layers[i+1]), mean=0.0, stddev=0.01), name=\"W_\"+str(i)))\n",
    "        \n",
    "        self.tf_saver = tf.train.Saver()\n",
    "        \n",
    "    def __gaussian_noise_layer(self, x):\n",
    "        noise = tf.random_normal(shape=tf.shape(x), mean=0.0, stddev=self.noise_std, dtype=tf.float32) \n",
    "        return x + noise\n",
    "    \n",
    "    def sample_forward(self, start_from=0, steps_num=None):\n",
    "        if(steps_num == None or steps_num > len(self.layers)):\n",
    "            steps_num = len(self.layers)-1\n",
    "        for i in range(start_from, start_from + steps_num):\n",
    "            mul = tf.matmul(self.layers[i][\"state\"], self.W[i]) + self.layers[i+1][\"b\"]\n",
    "            \n",
    "            if(self.unit_type == \"bin\") :\n",
    "                self.layers[i+1][\"state\"] = tf.nn.relu(tf.sign(tf.sigmoid(mul) \n",
    "                                                               - np.random.rand(tf.shape(mul)[0], tf.shape(mul)[1])))\n",
    "            else:\n",
    "                 self.layers[i+1][\"state\"] = tf.nn.relu(self.__gaussian_noise_layer(mul))\n",
    "                    \n",
    "        return self.layers[i+1][\"state\"]\n",
    "    \n",
    "    def sample_back(self, start_from=None, steps_count=None):\n",
    "        if(steps_count == None):\n",
    "            steps_count = len(self.layers)-1\n",
    "            start_from = len(self.layers)-1\n",
    "        i = start_from\n",
    "        while(i > start_from - steps_count):\n",
    "            mul = tf.matmul(self.layers[i][\"state\"], tf.transpose(self.W[i-1])) + self.layers[i-1][\"b\"]\n",
    "            if(self.unit_type == \"bin\"):\n",
    "                self.layers[i-1][\"state\"] = tf.nn.relu(tf.sign(tf.sigmoid(mul) \n",
    "                                                               - np.random.rand(tf.shape(mul)[0], tf.shape(mul)[1])))\n",
    "            else:\n",
    "                 self.layers[i-1][\"state\"] = self.__gaussian_noise_layer(mul)\n",
    "            i -= 1\n",
    "        return self.layers[i][\"state\"]\n",
    "    \n",
    "    def __getNextBatch(self, batch_size):\n",
    "        if(self.train_set):\n",
    "            if(callable(self.train_set)):\n",
    "                gen = self.train_set(batch_size)\n",
    "                for result in gen:\n",
    "                    yield result\n",
    "                    \n",
    "    def prepare_train_set(self, batch_size, epochs_count):\n",
    "        if(callable(self.train_set)):\n",
    "            return self.train_set(self.files, int(self.layers_nums[0]), batch_size, epochs_count, True)\n",
    "    \n",
    "    def train(self, train_set, batch_size, learning_rate, epochs_count, decrease_noise = 0, depth=None):\n",
    "        if(depth == None or depth > len(self.layers)-1):\n",
    "            depth = len(self.layers)\n",
    "        self.train_set = train_set\n",
    "        print(self.loaded_model)\n",
    "        if(self.loaded_model == False):\n",
    "            self.tf_sess = tf.Session()\n",
    "            self.tf_sess.run(tf.global_variables_initializer())\n",
    "        self.train_set = self.prepare_train_set(batch_size, epochs_count*(depth-1))\n",
    "        print(\"start training \"+str(learning_rate))\n",
    "        for i in range(depth-1):\n",
    "            self.noise_std = 0.001\n",
    "            for j in range(epochs_count):\n",
    "                self.noise_std = self.noise_std - decrease_noise*self.noise_std\n",
    "                self.layers[0][\"state\"] = self.input\n",
    "                h0_state = self.sample_forward(0, i+1)\n",
    "                v0_state = self.layers[i][\"state\"]\n",
    "                positive = tf.matmul(tf.transpose(v0_state), h0_state)\n",
    "                v1_state = self.sample_back(i+1, 1)\n",
    "                h1_state = self.sample_forward(i, 1)\n",
    "                negative = tf.matmul(tf.transpose(v1_state), h1_state)\n",
    "                w_update = self.W[i].assign_add(learning_rate*(positive - negative))\n",
    "                v_loss = tf.reduce_mean(v0_state - v1_state, 0)\n",
    "                vb_update = self.layers[i][\"b\"].assign_add(learning_rate*(v_loss))\n",
    "                hb_update = self.layers[i+1][\"b\"].assign_add(learning_rate*(tf.reduce_mean(h0_state - h1_state, 0)))\n",
    "                w_upd, bv_upd, bh_upd, loss = self.tf_sess.run([w_update, vb_update, hb_update, v_loss], feed_dict={\n",
    "                    self.input : norm(self.train_set.__next__())\n",
    "                })\n",
    "                if(j % 100 == 0):\n",
    "                    print(\"epoch: \"+str(j)+\" layer:\"+str(i)+\" loss:\")\n",
    "                    print(np.mean(np.power(loss, 2)))\n",
    "        self.train_set.__next__()\n",
    "        self.tf_saver.save(self.tf_sess, self.model_path)\n",
    "        \n",
    "    def load_model(self, path=None):\n",
    "        self.tf_sess = tf.Session()\n",
    "        self.tf_sess.run(tf.global_variables_initializer())\n",
    "        self.tf_saver.restore(self.tf_sess, path if path != None else self.model_path)\n",
    "        self.loaded_model == True\n",
    "        \n",
    "    def encode(self, data):\n",
    "        self.layers[0][\"state\"] = self.input\n",
    "        res = self.sample_forward()\n",
    "        with tf.Session() as self.tf_sess:\n",
    "           # print(\"WAT\")\n",
    "            self.tf_sess.run(tf.global_variables_initializer())\n",
    "            self.tf_saver.restore(self.tf_sess, self.model_path)\n",
    "            return self.tf_sess.run(res, feed_dict={\n",
    "                self.input : data\n",
    "            })\n",
    "    \n",
    "    def decode(self, data):\n",
    "        self.layers[-1][\"state\"] = self.input\n",
    "        res = self.sample_back()\n",
    "        with tf.Session() as self.tf_sess:\n",
    "            self.tf_sess.run(tf.global_variables_initializer())\n",
    "            self.tf_saver.restore(self.tf_sess, self.model_path)\n",
    "            return self.tf_sess.run(res, feed_dict={\n",
    "                self.input : data\n",
    "            })\n",
    "        \n",
    "    def loss(self):\n",
    "        self.layers[0][\"state\"] = self.input\n",
    "        return np.mean(np.pow(self.sample_back(self.sample_forvard(data) - self.input)))\n",
    "    \n",
    "    def finetune(self, train_set, batch_size, learning_rate, epochs_count):\n",
    "        self.train_set = train_set\n",
    "        if(self.decode_layers !=None):\n",
    "            self.decode_layers = []\n",
    "            for i in range(len(layers)-1):\n",
    "                self.decode_layers.append({\n",
    "                    \"state\" : None,\n",
    "                    \"b\" : self.layers[i][\"b\"]\n",
    "                })\n",
    "                i+=1\n",
    "\n",
    "            decode_W.W = []\n",
    "            for i in range(len(layers)-1):\n",
    "                self.decode_W.append(tf.transpose(self.W[i]))\n",
    "                \n",
    "        mimmaze = tf.train.AdamOptimizer(learning_rate).minimize(self.loss())\n",
    "        self.train_set = self.prepare_train_set(batch_size, len(self.layers))\n",
    "        \n",
    "        with tf.Session() as self.tf_sess:\n",
    "            self.tf_sess.run(tf.global_variables_initializer())\n",
    "            self.tf_saver.restore(self.tf_sess, self.model_path)\n",
    "            for i in range(epochs_count):\n",
    "                 self.tf_sess.run(mimmaze, feed_dict={\n",
    "                    self.input : norm(self.train_set.__next__())\n",
    "                })\n",
    "                \n",
    "    def compute_loss(self, data):\n",
    "        return np.mean(np.pow(self.decode(self.encode(data) - data)))\n",
    "    \n",
    "    def regression(self, train_data, epoch_count):\n",
    "        for i in range(layers_nums):\n",
    "            tf.stop_gradient(self.layers[\"b\"][i])\n",
    "            if(i < layers_nums):\n",
    "                tf.stop_gradient(self.W[i])\n",
    "        \n",
    "        self.rW = tf.Variable(tf.zeros([self.layers_nums[-1], len(self.files)]), name=\"rW\")\n",
    "        self.rb = tf.Variable(tf.zeros([len(self.files)]), name=\"rb\")\n",
    "        \n",
    "        if(self.loaded_model == False):\n",
    "            self.tf_sess = tf.Session()\n",
    "            self.tf_sess.run(tf.global_variables_initializer())\n",
    "        self.train_set = self.prepare_train_set(batch_size, epochs_count*(depth-1))\n",
    "            for j in range(epochs_count):\n",
    "                self.layers[0][\"state\"] = self.input[0]\n",
    "                res = self.sample_forward()\n",
    "                mul = tf.matmul(res, rW) + rb\n",
    "                cross_entropy = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(labels=self.input[1], logits=mul))\n",
    "                train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "                self.tf_sess.run(train_step, feed_dict = {\n",
    "                    self.input : norm(self.train_set)\n",
    "                })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm(x, get_mu_std=False):\n",
    "    ar = np.array(x)\n",
    "    mu = np.mean(ar)\n",
    "    std = np.std(ar)\n",
    "    if(get_mu_std):\n",
    "        return (ar - mu)/std, mu, std\n",
    "    else:\n",
    "        return (ar - mu)/std\n",
    "\n",
    "def restore(x, mu, std):\n",
    "    return np.array(x)*std + mu\n",
    "\n",
    "def get_file_frame(file_path, frame_with, frame_count, batch_count, rand=False, get_meta=False, get_label=False):\n",
    "    if(type(file_path ) == str):\n",
    "        wave_read = [wave.open(file_path, \"rb\")]\n",
    "    else:\n",
    "        wave_read = []\n",
    "        for fp in file_path:\n",
    "            wave_read.append (wave.open(fp, \"rb\"))\n",
    "    if(get_meta):\n",
    "        meta = {\n",
    "            \"nframes\" : wave_read[0].getnframes(),\n",
    "            \"nchannels\" : wave_read[0].getnchannels(),\n",
    "            \"sampwidth\" : wave_read[0].getsampwidth(),\n",
    "            \"framerate\" : wave_read[0].getframerate()\n",
    "        }\n",
    "    for k in range(batch_count):\n",
    "        out = []\n",
    "        labels = []\n",
    "        for i in range(frame_count):\n",
    "            label = np.random.randint(len(wave_read)) - 1\n",
    "            chosen_file = wave_read[label]\n",
    "            if rand:\n",
    "                chosen_file.setpos(np.random.randint(chosen_file.getnframes() - frame_with))           \n",
    "            if(get_label == True):\n",
    "                labels.append(label)\n",
    "            out.append(np.fromstring(chosen_file.readframes(frame_with), np.uint16))\n",
    "            yield [out, labels] if(get_label == True) else out\n",
    "    for fr in wave_read:\n",
    "        fr.close()\n",
    "    yield meta if get_meta else None\n",
    "    \n",
    "def write_wave(array, path, meta, mu=None, std=None):\n",
    "    unrolled = array.ravel()\n",
    "    writer = wave.open(path, \"wb\")\n",
    "    writer.setnframes(meta[\"nframes\"])\n",
    "    unrolled = restore(unrolled, mu, std)\n",
    "    bytes_arr = np.rint(unrolled).astype(np.uint16).tobytes()\n",
    "    writer.setnchannels(meta[\"nchannels\"])\n",
    "    writer.setsampwidth(meta[\"sampwidth\"])\n",
    "    writer.setframerate(meta[\"framerate\"])\n",
    "    writer.writeframes(bytes_arr)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "start training 1.5e-06\n",
      "epoch: 0 layer:0 loss:\n",
      "0.00185456\n",
      "epoch: 100 layer:0 loss:\n",
      "0.00200522\n",
      "epoch: 200 layer:0 loss:\n",
      "0.00287141\n",
      "epoch: 300 layer:0 loss:\n",
      "0.00215182\n",
      "epoch: 400 layer:0 loss:\n",
      "0.00141107\n",
      "epoch: 500 layer:0 loss:\n",
      "0.00102902\n",
      "epoch: 600 layer:0 loss:\n",
      "0.00105204\n",
      "epoch: 700 layer:0 loss:\n",
      "0.000923881\n",
      "epoch: 800 layer:0 loss:\n",
      "0.000986889\n",
      "epoch: 900 layer:0 loss:\n",
      "0.000868584\n",
      "epoch: 1000 layer:0 loss:\n",
      "0.000861197\n",
      "epoch: 1100 layer:0 loss:\n",
      "0.000831756\n",
      "epoch: 1200 layer:0 loss:\n",
      "0.000783817\n",
      "epoch: 1300 layer:0 loss:\n",
      "0.000783477\n",
      "epoch: 1400 layer:0 loss:\n",
      "0.000842742\n",
      "epoch: 1500 layer:0 loss:\n",
      "0.000749989\n",
      "epoch: 1600 layer:0 loss:\n",
      "0.000790337\n",
      "epoch: 1700 layer:0 loss:\n",
      "0.000785229\n",
      "epoch: 1800 layer:0 loss:\n",
      "0.000754832\n",
      "epoch: 1900 layer:0 loss:\n",
      "0.0007428\n",
      "epoch: 2000 layer:0 loss:\n",
      "0.000690783\n",
      "epoch: 2100 layer:0 loss:\n",
      "0.000714734\n",
      "epoch: 2200 layer:0 loss:\n",
      "0.000683934\n",
      "epoch: 2300 layer:0 loss:\n",
      "0.000783719\n",
      "epoch: 2400 layer:0 loss:\n",
      "0.000701644\n",
      "epoch: 2500 layer:0 loss:\n",
      "0.000726078\n",
      "epoch: 2600 layer:0 loss:\n",
      "0.000599243\n",
      "epoch: 2700 layer:0 loss:\n",
      "0.00064338\n",
      "epoch: 2800 layer:0 loss:\n",
      "0.000697245\n",
      "epoch: 2900 layer:0 loss:\n",
      "0.000641906\n",
      "epoch: 3000 layer:0 loss:\n",
      "0.000641706\n",
      "epoch: 3100 layer:0 loss:\n",
      "0.000815253\n",
      "epoch: 3200 layer:0 loss:\n",
      "0.000581732\n",
      "epoch: 3300 layer:0 loss:\n",
      "0.000737514\n",
      "epoch: 3400 layer:0 loss:\n",
      "0.000607613\n",
      "epoch: 3500 layer:0 loss:\n",
      "0.000651243\n",
      "epoch: 3600 layer:0 loss:\n",
      "0.000621062\n",
      "epoch: 3700 layer:0 loss:\n",
      "0.000625699\n",
      "epoch: 3800 layer:0 loss:\n",
      "0.000600898\n",
      "epoch: 3900 layer:0 loss:\n",
      "0.000656713\n",
      "epoch: 4000 layer:0 loss:\n",
      "0.000608869\n",
      "epoch: 4100 layer:0 loss:\n",
      "0.000639309\n",
      "epoch: 4200 layer:0 loss:\n",
      "0.000583396\n",
      "epoch: 4300 layer:0 loss:\n",
      "0.000721412\n",
      "epoch: 4400 layer:0 loss:\n",
      "0.000651484\n",
      "epoch: 4500 layer:0 loss:\n",
      "0.000549667\n",
      "epoch: 4600 layer:0 loss:\n",
      "0.000688944\n",
      "epoch: 4700 layer:0 loss:\n",
      "0.000617068\n",
      "epoch: 4800 layer:0 loss:\n",
      "0.000579679\n",
      "epoch: 4900 layer:0 loss:\n",
      "0.000617374\n",
      "epoch: 5000 layer:0 loss:\n",
      "0.000628213\n",
      "epoch: 5100 layer:0 loss:\n",
      "0.00064905\n",
      "epoch: 5200 layer:0 loss:\n",
      "0.000582886\n",
      "epoch: 5300 layer:0 loss:\n",
      "0.000714318\n",
      "epoch: 5400 layer:0 loss:\n",
      "0.000568839\n",
      "epoch: 5500 layer:0 loss:\n",
      "0.0005936\n",
      "epoch: 5600 layer:0 loss:\n",
      "0.000705452\n",
      "epoch: 5700 layer:0 loss:\n",
      "0.000623257\n",
      "epoch: 5800 layer:0 loss:\n",
      "0.0006246\n",
      "epoch: 5900 layer:0 loss:\n",
      "0.000531752\n",
      "epoch: 6000 layer:0 loss:\n",
      "0.000540045\n",
      "epoch: 6100 layer:0 loss:\n",
      "0.000604425\n",
      "epoch: 6200 layer:0 loss:\n",
      "0.000655591\n",
      "epoch: 6300 layer:0 loss:\n",
      "0.000550354\n",
      "epoch: 6400 layer:0 loss:\n",
      "0.000556647\n",
      "epoch: 6500 layer:0 loss:\n",
      "0.000511023\n",
      "epoch: 6600 layer:0 loss:\n",
      "0.000582413\n",
      "epoch: 6700 layer:0 loss:\n",
      "0.00061048\n",
      "epoch: 6800 layer:0 loss:\n",
      "0.000643707\n",
      "epoch: 6900 layer:0 loss:\n",
      "0.000543335\n",
      "[[-0.07852621  0.04303839  0.00936075 ...,  0.00845565 -0.00661216\n",
      "   0.05168602]\n",
      " [-0.03718904  0.03056057  0.02117441 ...,  0.01387404 -0.00089019\n",
      "   0.04024903]\n",
      " [-0.04350831  0.02475551  0.03371805 ..., -0.0035981  -0.0048232\n",
      "   0.04609442]\n",
      " ..., \n",
      " [ 0.05192529 -0.01566724  0.00261182 ...,  0.008178   -0.07565701\n",
      "   0.0045033 ]\n",
      " [ 0.06095596 -0.01795447  0.01747073 ...,  0.0461764  -0.07044598\n",
      "  -0.00527396]\n",
      " [ 0.06749827 -0.02750879  0.04700766 ...,  0.03357461 -0.04597913\n",
      "  -0.00106988]]\n",
      "epoch: 0 layer:1 loss:\n",
      "0.21625\n",
      "epoch: 100 layer:1 loss:\n",
      "0.000665891\n",
      "epoch: 200 layer:1 loss:\n",
      "0.000705431\n",
      "epoch: 300 layer:1 loss:\n",
      "0.000826476\n",
      "epoch: 400 layer:1 loss:\n",
      "0.000734656\n",
      "epoch: 500 layer:1 loss:\n",
      "0.000589245\n",
      "epoch: 600 layer:1 loss:\n",
      "0.000550249\n",
      "epoch: 700 layer:1 loss:\n",
      "0.000466202\n",
      "epoch: 800 layer:1 loss:\n",
      "0.000542648\n",
      "epoch: 900 layer:1 loss:\n",
      "0.000485341\n",
      "epoch: 1000 layer:1 loss:\n",
      "0.000469321\n",
      "epoch: 1100 layer:1 loss:\n",
      "0.000506044\n",
      "epoch: 1200 layer:1 loss:\n",
      "0.000414951\n",
      "epoch: 1300 layer:1 loss:\n",
      "0.000398842\n",
      "epoch: 1400 layer:1 loss:\n",
      "0.000391531\n",
      "epoch: 1500 layer:1 loss:\n",
      "0.000392237\n",
      "epoch: 1600 layer:1 loss:\n",
      "0.00043083\n",
      "epoch: 1700 layer:1 loss:\n",
      "0.000334875\n",
      "epoch: 1800 layer:1 loss:\n",
      "0.000466137\n",
      "epoch: 1900 layer:1 loss:\n",
      "0.000386742\n",
      "epoch: 2000 layer:1 loss:\n",
      "0.000325609\n",
      "epoch: 2100 layer:1 loss:\n",
      "0.000306463\n",
      "epoch: 2200 layer:1 loss:\n",
      "0.00032148\n",
      "epoch: 2300 layer:1 loss:\n",
      "0.000318602\n"
     ]
    }
   ],
   "source": [
    "rbm = DBM([500, 500, 250], \"gauss\")\n",
    "rbm.files= [\"01.wav\", \"03.wav\", \"04.wav\", \"05.wav\", \"06.wav\", \"07.wav\", \"11.wav\" ,\"13.wav\", \"14.wav\"]\n",
    "rbm.train(get_file_frame, 500, 0.0000015, 7000, 0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm = DBM([500, 700, 400], \"gauss\")\n",
    "gen = get_file_frame(\"01.wav\", 500, 500, 1, False, True)\n",
    "foo = gen.__next__()  \n",
    "foo, mu, std = norm(foo, True)\n",
    "res = rbm.encode(foo) \n",
    "meta = gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoded = rbm.decode(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_wave(decoded, \"ress.wav\", meta, mu, std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2485055 , -0.62448007, -0.50707173, ..., -0.51627713,\n",
       "        -0.38464558, -0.21760647],\n",
       "       [-0.0380106 , -0.5878644 , -0.42959678, ..., -0.79777837,\n",
       "        -0.49409249, -1.04269648],\n",
       "       [ 1.06251526, -0.31583071, -0.21873406, ...,  0.27678546,\n",
       "         0.13807897,  0.05830932],\n",
       "       ..., \n",
       "       [-0.44847888, -0.70732969, -0.56549644, ..., -1.13774168,\n",
       "        -1.11916137, -0.80218136],\n",
       "       [ 0.32676309,  0.00529569,  0.19926089, ..., -0.14910096,\n",
       "        -1.17060566, -0.31726819],\n",
       "       [-0.32813689, -0.6678564 ,  0.69116747, ...,  0.25979102,\n",
       "        -0.63806701, -0.47313672]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trim_zeros([0,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array([2,0,3]) != 0"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
